defaults:
  - base_mnist
  - _self_

task:
  _target_: delete_unconditional.DeleteUnconditional

project_name: mnist-deletion-hf-test
output_dir: checkpoints/mnist/deletion
checkpoint_path: checkpoints/mnist/retrain/2024-05-20_00-55-12/checkpoint-105000

deletion:
  class_label: 4 # class to remove
  frac_deletion: 0.1 # each class is 10% of dataset
  loss_fn: importance_sampling_with_mixture # Options are [importance_sampling_with_mixture, double_forward_with_neg_del, naive_del, simple_neg_del, modified_noise_obj]
  timestep_del_window: null # only for modified_noise_obj (500 is good)
  loss_params: # {}
    superfactor: 1.6
    lambd: 0.5
  superfactor_decay: null
  # IGNORE ALL BELOW: just placeholders for running sweep
  inception_frequency: null 
  training_steps: null

subfolders:
  unet: unet_ema

metrics:
  classifier_cfg:
    _target_: metrics.classifier.Classifier
    classifier:
      _target_: hydra.utils.get_object
      path: metrics.mnist_resnet.resnet18
    classifier_ckpt: checkpoints/classifiers/mnist.pt
    classifier_args: 
      num_classes: 10
      grayscale: true
    transform: null
  fraction_deletion: true
  inception_score: 
    class_cfg: 
      _target_: metrics.inception_score.InceptionScore
      remove_class: ${deletion.class_label}
    step_frequency: 25
    num_imgs_to_generate: 16384
    batch_size: 4096
  fid: null
  # membership_loss:
  #   class_cfg:
  #     _target_: metrics.class_membership.MembershipLoss
  #     golden_timestep: 200 
  #     num_image_samples: 128 #1024
  #     num_noise_samples: 128 #128
  #     eval_batch_size: 4096
  #   step_frequency: 1 # re-evaluate every gradient step
  membership_loss:
    class_cfg:
      _target_: metrics.class_membership.MembershipLoss
      num_image_samples: 64 #1024
      num_noise_samples: 64 #128
      eval_batch_size: 4096
    timesteps: [200, 400] #[200, 400, 600, 800, 900]
    plot_params: null
    #   save_path: 'cifar_losses.png'
    #   time_frequency: 10 # initial graph for all membership losses
    step_frequency: 1 # re-evaluate every gradient step

ema:
  use_ema: false # Unnecessary for deletion fine-tuning (only need to load in ema weights from training at start)

sampling_steps: 1 # Specifies number of gradient steps after which to evaluate
checkpointing_steps: 50 # Specifies number of gradient steps after which to save checkpoint
training_steps: 300
warmup_steps: 0 # for lr scheduler
eval_batch_size: 144

dataset_all:
  _target_: data.src.hf_dataset.HFDataset
  filter: nondeletion
  name: mnist
  split: train
  class_to_remove: ${deletion.class_label}
  image_key: ${dataset.image_key}

dataset_deletion:
  _target_: data.src.hf_dataset.HFDataset
  filter: deletion
  name: mnist
  split: train
  class_to_remove: ${deletion.class_label}
  image_key: ${dataset.image_key}