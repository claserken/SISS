random_seed: 42
project_name: mnist-retrain-hf
output_dir: checkpoints/mnist/retrain

task:
  _target_: train_unconditional.TrainUnconditional

dataset:
  _target_: data.src.hf_dataset.HFDataset
  filter: nondeletion
  name: mnist
  split: train
  image_key: image
  class_to_remove: 4
  # convert_to_rgb: false

transform:
  _target_: torchvision.transforms.Compose
  transforms:
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize # Normalize to range [-1, 1]
      mean: [0.5] 
      std: [0.5] 

resolution: 28
unet:
  _target_: diffusers.UNet2DModel
  sample_size: ${resolution}
  in_channels: 1
  out_channels: 1
  block_out_channels: 
    - 64
    - 128
    - 256
  down_block_types:
    - DownBlock2D
    - AttnDownBlock2D
    - DownBlock2D
  up_block_types:
    - UpBlock2D
    - AttnUpBlock2D
    - UpBlock2D

scheduler:
  _type: initialize
  _target_: diffusers.DDPMScheduler
  num_train_timesteps: 1000
  beta_schedule: squaredcos_cap_v2 # linear can also work here
  prediction_type: "epsilon" # Choices: "epsilon", "sample"

subfolders: # for loading during deletion later
  unet_ema: unet_ema # if use_ema is false then this should be unet_ema, otherwise just unet
  unet: unet
  noise_scheduler: null # scheduler

pipeline:
  num_inference_steps: 50

enable_xformers_memory_efficient_attention: false

lr_scheduler: cosine
lr_warmup_steps: 500
gradient_accumulation_steps: 1
mixed_precision: null # Choices: null (defaults to fp32), "fp16", "bf16". Bf16 requires an Nvidia Ampere GPU or newer
checkpointing_steps: 2500 # Save a checkpoint every N updates. For resuming using "resume_from_checkpoint"
resume_from_checkpoint: null
sampling_steps: 50 # Sample from model every N updates for logging
checkpoints_total_limit: null # Max number of checkpoints to keep
checkpoint_path: null # Use a path saved by checkpointing_steps or "latest" to automatically select the last available
center_crop: false
random_flip: false
train_batch_size: 128 # Makes 500 steps roughly 1 epoch
eval_batch_size: 64
dataloader_num_workers: 0
num_epochs: 250 # should be increased significantly and trained for at least a few hours roughly
# save_images_epochs: 10
# save_model_epochs: 10

optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-4
  betas: 
    - 0.95
    - 0.999
  weight_decay: 1e-6
  eps: 1e-8

logging:
  logger: wandb # Choices: "wandb", "tensorboard" (Wandb is significantly better supported)
  logging_dir: logs

ema:
  use_ema: true
  ema_inv_gamma: 1.0
  ema_power: 0.75
  ema_max_decay: 0.9999